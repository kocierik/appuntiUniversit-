\documentclass[12pt]{article}
\usepackage{amsfonts, amssymb, amsmath}
\parindent 0px

\begin{document}
\title{Algoritmi e strutture dati}
\author{Koci Erik}
\maketitle
\section{Complessità algoritmi}
\begin{enumerate}
    \item \textbf{Costo:} si riferisce al costo di un singolo algoritmo
\item \textbf{Complessità:} si riferisce a più risoluzioni di un algoritmo 
\end{enumerate}

il costo di un blocco \textbf{if-then-else} è  $O(max\{f(n),g(n),h(n)\})$ cioè \textbf{O(1)}.

\subsection{Ordini di grandezza}
\begin{enumerate}
    \item $\Theta(f(n))$ se \textbf{cresce tanto quanto f}
    \item $O(f(n))$ \textbf{se la crescita è minore o uguale a f}
    \item $\Omega(f(n))$ \textbf{se la crescita è maggiore o uguale a f}
\end{enumerate}
\subsection{Esercizi}
\begin{itemize}
    \item $1325 n^2 + 12 n + 1 = \theta(n^3)$ FALSO
    \item $76 n^3 = =(n^3)$ VERO
    \item $n^2 log n = O(n^2)$ FALSO
    \item $3^N = O(2^N)$ FALSO
    \item $1^n = O(2^\frac{n}{2})$ FALSO
    \item $2^N+100 = O(2^N)$ VERO
    \item $n = O(n log n)$ VERO
    \item $n^2 = (n log n$ FALSO
    \item $log(n^2) = \Theta(log n)$ VERO
    \item $(n+1)/2 = \Theta(n)$ VERO
    \item $\frac{(n+1)*n}{2} = \Theta(n^2)$ VERO
\end{itemize}
\subsection{Analisi casi}
\begin{enumerate}
    \item \textbf{Caso pessimo} $$T\textsc{worst}(n) = max T(I)$$
    \item \textbf{Caso ottimo} $$T\textsc{best}(n) = min T(I)$$
    \item \textbf{Caso medio} $$T\textsc{avg}(n) = \sum\limits_{} T(I)P(I)$$
\end{enumerate}
\subsection{Algoritmi ordinamento}
\textbf{Selection sort:} scorre tutti gli elementi degli array e si cerca il valore più piccolo scambiando i due valori. Il costo è lineare con il numero di elementi da considerare:
$$T(n) = \Theta (n^2)$$
\textbf{il costo è $\Theta(n^2)$} perchè è presente una funzione min che ogni volta controlla se il numero è il minore.
Le chiamate a \textbf{min} contribuiscono a $n^2$ mentre il resto combacia con $n$
cioè $n^2+n$;\,\,\,\,n viene assorbito. \\

\textbf{Ricerca binaria (ricorsiva):} per utilizzare questo algoritmo devo avere un array ordinato. Cerco il valore andando a verificare sempre nella metà dove mi aspetto che sia presente.
$$T(n) = 1 \,\,\,se\,\, n=0$$
$$T(n) = T(n/2)+1 $$ 
\textbf{equazione di ricorrenza:} ci aiuta a calcolare il costo analizzando una singola ricorsione. \\\\
\subsection{metodo dell'iterazione:} consiste nello sviluppare l'equazione di ricorrenza, per intuirne la soluzione. 

$$T(n) = c_1 + c_2*log (n) = \Theta(log (n))$$
E' presente il \textbf{logaritmo} perchè ogni volta devo \textbf{dimezzare} il tutto in base al numero di elementi. $c_1$ perchè devo eseguire le istruzioni la prima volta.\\

dimostrare che $T(n) = O(n)$
$$T(n = 1) \,\, n==1$$
$$T(T([n/2])+n \,\,\, n>1$$
\subsection{Metodo della sostituzione:} consiste facendo una dimostrazione per induzione. quindi parto dal valore base che è n (esempio 1) e dimostriamo che vale anche per un $n$ più grande. \\\\
\textbf{caso base:} $$T(1) = 1 \leq c x 1$$ \\
\textbf{induzione:}
$$T(n) = T([n/2])+n$$ $$ \leq c[n/2]+n \,\,\,\,\, (ipotesi\,induttiva)$$
$$\leq cn/2+n=\frac{cn+2n}{2} = (c/2+1)n \leq cn$$ \\

\subsection{Master Theorem:} Si consideri la seguente equazione di ricorrenza: \\
$$T(n) = d \,\,\,se\,\, n=1$$
$$T(n) = aT(n/b)+cn^\beta \,\,\,se\,\, n>1$$
e sia: $$\alpha = \frac{log (a)}{log(b)}$$

\textbf{a} numero di chiamate ricorsive\\
\textbf{b} mi dice come partiziono il mio input\\
questi due valori mi danno $\alpha$. \\
$\beta$ mi dice l'esponente che avevo. \\

L'equazione di ricorrenza ha la seguente soluzione:
\begin{enumerate}
    \item $T(n) = \Theta(n^\alpha)$ se $\alpha > \beta$
    \item $T(n) = \Theta(n^\alpha*log(n))$ se $\alpha = \beta$
    \item $T(n) = \Theta(n^\beta)$ se $\alpha < \beta$
\end{enumerate}


Il teorema fondamentale  \textbf{non} si può applicare ad algoritmi ricorsivi che non effettuano \textbf{partizioni bilanciate}.\\
ad esempio non può essere applicato nella risoluzione di fibonacci ricorsivo.\\

Esempio: \\
$$T(n) = 1 \, n<=1$$
$$T(n−1)+T(n − 2)+1 \, n > 2$$
Se le \textbf{partizioni sono bilanciate} conviene utilizzare il   \textbf{Master Theorem}.\\\\
\textbf{partizione bilanciate:} quando facciamo chiamate ricorsive prendo il mio input suddividendolo in parti n/b. Fibonacci non è bilancianto perche abbiamo due chiamate ricorsive diverse. \\

\textbf{L’analisi ammortizzata:} studia il costo medio di una sequenza di operazioni.\\

Sia  \textbf{T(n, k)} il tempo totale richiesto da un algoritmo, nel caso pessimo, per effettuare k operazioni su istanze di lunghezza n.
Definiamo il \textbf{costo ammortizzato} su una sequenza di k operazioni come:
$$T_\alpha(n)=\frac{T(n,k)}{k}$$


\subsection{Algoritmi di visita degli alberi}
Esistono due tipologie di visita:
\begin{itemize}
    \item In profondità (pre-ordine, in-rodine, post-ordine)
    \item In ampiezza
\end{itemize}
Nella visita \textbf{pre-ordine} si parte visitando il nodo della radice per poi passare a visitare tutto il nodo sinistro, risalendo poi andando verso destra.\\\\
Nella visita \textbf{in-ordine} si parte a visitare dal ramo più in basso a sinistra risalendo per poi andare verso destra. \\\\
Nella visita \textbf{post-ordine} vengono prima visitati i nodi più in profondità partendo sempre da sinistra verso destra per poi risalire.\\\\
Nella visita per \textbf{ampiezza} si analizza l'albero a livelli, partendo dalla radice.

\subsection{Alberi AVL}
Un albero $AVL$ è un albero di ricerca (quasi) bilanciato. Questo albero supporta le operazioni di $insert(), delete(), search()$ con costo $O(log n)$ nel \textbf{caso pessimo}.

\subsubsection{Fattore di bilanciamento}
Il fattore di bilanciamento $\beta(v)$ di un novo $v$ è dato dalla differenza tra l'altezza del sottoalbero sinistro e del sottoalbero destro di v:
$$\beta(v) = altezza(v.left)-altezza(v.right)$$
\subsubsection{Bilanciamento in altezza}
Un albero si dice \textbf{bilanciato in altezza} se le altezze dei sottoalberi sinitro e destro di ogni nodo differiscono al più di uno.
$$\beta\leq1$$
\textbf{Definizione:} un albero $AVL$ è un $ABR$ bilanciato in altezza.
\subsubsection{Inserimento e rimozione} 
Inserimenti e rimozioni richiedono di essere modificati per mantenere il bilanciamento dell'albero. \\
L'operazione fondamentale per ribilanciare l'albero è la \textbf{rotazione semplice}.\\
\subsubsection{Rotazione a sinistra}
Per effettuare questa rotazione prendo il nodo problematico e effettuo una rotazione scambiandolo con il successivo ed il nodo scambiato diventerà figlio destro mentre il figlio del nodo precendete diventerà figlio sinistro.

\subsection{Alberi 2-3}
Un albeero 2-3 è un alvero in cui:
\begin{itemize}
    \item Tutti i percorsi radice-foglia hanno la stessa lunghezza
    \item Le foglie contengono le chiavi (e i dati da memorizzare) e sono ordinate da sinistra verso destra in ordine di chiave crescente.
    \item Ogni nodo interno (non folgia) v ha 2 o 3 figli e mantiene due informazioni \begin{itemize}
        \item S[v], \textbf{chiave massima} nel sottoalbero sinitro (2 o 3 figli)
        \item M[v], \textbf{chiave massima} nel sottoalbero centrale (3 figli)
    \end{itemize}
    \item Distribuzione dei valori $k$ delle chiavi nei sottoalberi: \begin{itemize}
        \item Sinistro $k \leq S[v]$
        \item Centro $S[v] < k \leq M[v]$
        \item Destro $k > M[v]$
    \end{itemize}
\end{itemize}


\subsection{Tabelle Hash}
Le \textbf{tabelle hash} hanno una implementazione basata su una chiave $k$ e array.\\
Per ottenere la chiave sono presenti diverse tecniche di calcolo.\\
Ricapitolando, per realizzare una tabella hash efficiente abbiamo bisogno di:
\begin{itemize}
    \item Un vettore
    \item Una funzione has calcolabile velocemente e che garantisca una buona distribuzione delle chiavi nel vettore
    \item Un meccanisco per gestire le collisioni 
\end{itemize}
\subsubsection{Problema delle collisioni}
Una funzione hash $h$ si dice \textbf{perfetta} se è iniettiva:
$$\forall u, \in U: u \neq v \xrightarrow{} h(u) \neq h(v)$$
Se le collisioni sono inevitabili, è necessario trovare un metodo che le minimizzi, distribuendo \textbf{uniformemente} le chaivi negli indici della tabella hash.
\subsubsection{Funzioni hash}
E' necessario fare una premessa; nelle funzioni hash è \textbf{sempre possibile} trasformare una chiave complessa in un numero, (conversione in binario).
\subsubsection{Metodo dell'estrazione}
Le caratteristiche di questo metodo sono:
\begin{itemize}
    \item Usa solo una parte della chiave
    \item Si seleziona una sottosequenza di $p$ bit, con $m = 2^p$
    \item Solitamente dalle posizioni centrali
\end{itemize}
\textbf{esempio:} Verranno prese le cifre centrali 101000
\begin{center}
 bin(“beer”) = 000010 000101 000101 010010 
\end{center}
\begin{itemize}
    \item \textbf{Vantaggi:} molto veloce da calcolare
    \item \textbf{Svantaggi} rischio collisioni più alto di altri metodi
\end{itemize}

\subsubsection{Metodo della divisione}
Basata sul resto della divisione per $m$:
\begin{itemize}
    \item \textbf{Vantaggio:} molto veloce
    \item \textbf{Svantaggio:} Suscettibile a speficifi valori di $m$. Per risolvere questo problema bisogna scegliere $m$ come numero primo non troppo vicino ad una potenza di 2.
\end{itemize}
\textbf{Esempio:}
$$m=12, k=100 \xrightarrow{} h(k)=4$$
\subsubsection{Metodo della moltiplicazione}
Basato sulla moltiplicazione e il resto del numero
\begin{enumerate}
    \item Sia $A$ una costante, $0 < A < 1$
    \item Moltiplichiamo $k$ per $A$ e prendiamo la parte frazionaria
    \item Moltiplichiamo quest'ultima per $m$ e prendiamo la parte intera
\end{enumerate}
\textbf{Esempi:}
$$m = , k = 3, A = 0.8 \xrightarrow{} h(k) = 2$$
$$m = 1000,k = 123, A \approx 0.6180339887... \xrightarrow{} h(k) = 18$$
\begin{itemize}
    \item \textbf{Svantaggi:} lento (più lento del metodo di divisione)
    \item \textbf{Vantaggi} Il valore di $m$ non è critico
    \item \textbf{Come scegliere A?} $A \approx (\sqrt{5}-1)/2 = 0.61803...$ \textbf{(Knuth)}
\end{itemize}
\subsubsection{Metodo della codifica algebrica}
Metodo utilizzando dal compilatore java basato su espressioni algebriche:
$$h(k) = (k_n x^n + k_n_-_1 x^n^-^1 + ... + k_1 x + k_0) mod \hspace{0.1cm}m$$
$$k = k_n k_n_-_1 ... k_1k_0$$
Dove $k_0, k_1...$ possono essere, ad esempio, i bit della \textbf{codifica binaria} di $k$, oppure i \textbf{codici ascii} dei singoli caratteri di $k$.\\ $x$ è un valore \textbf{costante}.
\begin{itemize}
    \item \textbf{Vantaggi:} dipende da tutti i bit/caratteri della chiave
    \item \textbf{Svantaggi:} $n$ addizioni e $n*(n-1)/2$ prodotti
\end{itemize}
\subsubsection{Problema delle collisioni}
Attraverso questi metodi elencati precedentemente siamo riusciti a ridurre il numero di collisioni, ma senza eliminarle.\\
Per risolvere questo problema la  \textbf{complessita computazionale potrebbe aumentare a $n$}, possono essere utilizzate le seguenti tecniche:
\begin{enumerate}
    \item \textbf{Concatenamento}
    \item \textbf{Indirizzamento aperto}
\end{enumerate}
\subsubsection{concatenamento}
     Nella tecnica di  \textbf{concatenameto} gli elementi con lo stesso valore hash  $h$ vengono memorizzati in una lista concatenata (linked list).\\

Il \textbf{fattore di carico} è dato dal rapporto tra numero di elementi memorizzati e dimensioni della tabella. \\\\
La \textbf{complessità} del concatenamento è la seguente:
\begin{itemize}
    \item insert: $\Theta(1)$
    \item search: $\Theta(n)$
    \item delete: $\Theta(n)$
\end{itemize}
\subsubsection{indirizzamento aperto}

\section{Scelta degli algoritmi}
A seconda delle operazioni da eseguire è necessario adattare diverse tecniche di implimentazione di un algoritmo.
\subsubsection{Implementazione su un vettore ordinato} 
Questo tipo di ricerca ha un costo computazionale basso nel caso in cui volessimo \textbf{ricercare degli elementi}.
Costi computazionali:
\begin{itemize}
    \item Ricerca $O(log\hspace{0.1cm}n)$
    \item Inserimento $O(n)$
    \item Eliminazione $O(n)$
\end{itemize}
\subsubsection{Implementazione su liste concatenate non ordinate}
Questo implementazione converrebbe utilizzarla nel caso in cui volessimo \textbf{aggiungere o eliminare degli elementi}.
Costi computazionali:
\begin{itemize}
    \item Ricerca $O(n)$
    \item Inserimento $O(1)$
    \item Eliminazione $O(n)$
\end{itemize}

\subsubsection{Implementazione alberi ABR}
Implementazione basata su alberi binari.
Costi computazionali:
\begin{itemize}
    \item Ricerca $O(h)$
    \item Inserimento $O(h)$
    \item Eliminazione $O(h)$
\end{itemize}

\subsubsection{Implementazione alberi AVL}
Implementazione basata su alberi binari a altezza equivalente.
Costi computazionali:
\begin{itemize}
    \item Ricerca $O(log \hspace{0.1cm}n)$
    \item Inserimento $O(log \hspace{0.1cm}n)$
    \item Eliminazione $O(log \hspace{0.1cm}n)$
\end{itemize}

\subsubsection{Implementazione alberi 2-3}
Implementazione basata su alberi binari ordinati con chiavi massime.
Costi computazionali:
\begin{itemize}
    \item Ricerca $O(log \hspace{0.1cm}n)$
    \item Inserimento $O(log \hspace{0.1cm}n)$
    \item Eliminazione $O(log \hspace{0.1cm}n)$
\end{itemize}



\end{document}