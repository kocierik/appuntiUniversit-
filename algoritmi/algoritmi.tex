\documentclass[12pt]{article}
\usepackage{amsfonts, amssymb, amsmath}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\parindent 0px

\begin{document}
\title{Algoritmi e strutture dati}
\author{Koci Erik}
\maketitle
\section{Complessità algoritmi}
\begin{enumerate}
    \item \textbf{Costo:} si riferisce al costo di un singolo algoritmo
\item \textbf{Complessità:} si riferisce a più risoluzioni di un algoritmo 
\end{enumerate}
\includegraphics{universe}
il costo di un blocco \textbf{if-then-else} è  $O(max\{f(n),g(n),h(n)\})$ cioè \textbf{O(1)}.

\subsection{Ordini di grandezza}
\begin{enumerate}
    \item $\Theta(f(n))$ se \textbf{cresce tanto quanto f}
    \item $O(f(n))$ \textbf{se la crescita è minore o uguale a f}
    \item $\Omega(f(n))$ \textbf{se la crescita è maggiore o uguale a f}
\end{enumerate}
\subsection{Esercizi}
\begin{itemize}
    \item $1325 n^2 + 12 n + 1 = \theta(n^3)$ FALSO
    \item $76 n^3 = =(n^3)$ VERO
    \item $n^2 log n = O(n^2)$ FALSO
    \item $3^N = O(2^N)$ FALSO
    \item $1^n = O(2^\frac{n}{2})$ FALSO
    \item $2^N+100 = O(2^N)$ VERO
    \item $n = O(n log n)$ VERO
    \item $n^2 = (n log n$ FALSO
    \item $log(n^2) = \Theta(log n)$ VERO
    \item $(n+1)/2 = \Theta(n)$ VERO
    \item $\frac{(n+1)*n}{2} = \Theta(n^2)$ VERO
\end{itemize}
\subsection{Analisi casi}
\begin{enumerate}
    \item \textbf{Caso pessimo} $$T\textsc{worst}(n) = max T(I)$$
    \item \textbf{Caso ottimo} $$T\textsc{best}(n) = min T(I)$$
    \item \textbf{Caso medio} $$T\textsc{avg}(n) = \sum\limits_{} T(I)P(I)$$
\end{enumerate}
\subsection{Algoritmi ordinamento}
\textbf{Selection sort:} scorre tutti gli elementi degli array e si cerca il valore più piccolo scambiando i due valori. Il costo è lineare con il numero di elementi da considerare:
$$T(n) = \Theta (n^2)$$
\textbf{il costo è $\Theta(n^2)$} perchè è presente una funzione min che ogni volta controlla se il numero è il minore.
Le chiamate a \textbf{min} contribuiscono a $n^2$ mentre il resto combacia con $n$
cioè $n^2+n$;\,\,\,\,n viene assorbito. \\

\textbf{Ricerca binaria (ricorsiva):} per utilizzare questo algoritmo devo avere un array ordinato. Cerco il valore andando a verificare sempre nella metà dove mi aspetto che sia presente.
$$T(n) = 1 \,\,\,se\,\, n=0$$
$$T(n) = T(n/2)+1 $$ 
\textbf{equazione di ricorrenza:} ci aiuta a calcolare il costo analizzando una singola ricorsione. \\\\
\subsection{metodo dell'iterazione:} consiste nello sviluppare l'equazione di ricorrenza, per intuirne la soluzione. 

$$T(n) = c_1 + c_2*log (n) = \Theta(log (n))$$
E' presente il \textbf{logaritmo} perchè ogni volta devo \textbf{dimezzare} il tutto in base al numero di elementi. $c_1$ perchè devo eseguire le istruzioni la prima volta.\\

dimostrare che $T(n) = O(n)$
$$T(n = 1) \,\, n==1$$
$$T(T([n/2])+n \,\,\, n>1$$
\subsection{Metodo della sostituzione:} consiste facendo una dimostrazione per induzione. quindi parto dal valore base che è n (esempio 1) e dimostriamo che vale anche per un $n$ più grande. \\\\
\textbf{caso base:} $$T(1) = 1 \leq c x 1$$ \\
\textbf{induzione:}
$$T(n) = T([n/2])+n$$ $$ \leq c[n/2]+n \,\,\,\,\, (ipotesi\,induttiva)$$
$$\leq cn/2+n=\frac{cn+2n}{2} = (c/2+1)n \leq cn$$ \\

\subsection{Master Theorem:} Si consideri la seguente equazione di ricorrenza: \\
$$T(n) = d \,\,\,se\,\, n=1$$
$$T(n) = aT(n/b)+cn^\beta \,\,\,se\,\, n>1$$
e sia: $$\alpha = \frac{log (a)}{log(b)}$$

\textbf{a} numero di chiamate ricorsive\\
\textbf{b} mi dice come partiziono il mio input\\
questi due valori mi danno $\alpha$. \\
$\beta$ mi dice l'esponente che avevo. \\

L'equazione di ricorrenza ha la seguente soluzione:
\begin{enumerate}
    \item $T(n) = \Theta(n^\alpha)$ se $\alpha > \beta$
    \item $T(n) = \Theta(n^\alpha*log(n))$ se $\alpha = \beta$
    \item $T(n) = \Theta(n^\beta)$ se $\alpha < \beta$
\end{enumerate}


Il teorema fondamentale  \textbf{non} si può applicare ad algoritmi ricorsivi che non effettuano \textbf{partizioni bilanciate}.\\
ad esempio non può essere applicato nella risoluzione di fibonacci ricorsivo.\\

Esempio: \\
$$T(n) = 1 \, n<=1$$
$$T(n−1)+T(n − 2)+1 \, n > 2$$
Se le \textbf{partizioni sono bilanciate} conviene utilizzare il   \textbf{Master Theorem}.\\\\
\textbf{partizione bilanciate:} quando facciamo chiamate ricorsive prendo il mio input suddividendolo in parti n/b. Fibonacci non è bilancianto perche abbiamo due chiamate ricorsive diverse. \\

\textbf{L’analisi ammortizzata:} studia il costo medio di una sequenza di operazioni.\\

Sia  \textbf{T(n, k)} il tempo totale richiesto da un algoritmo, nel caso pessimo, per effettuare k operazioni su istanze di lunghezza n.
Definiamo il \textbf{costo ammortizzato} su una sequenza di k operazioni come:
$$T_\alpha(n)=\frac{T(n,k)}{k}$$


\subsection{Algoritmi di visita degli alberi}
Esistono due tipologie di visita:
\begin{itemize}
    \item In profondità (pre-ordine, in-rodine, post-ordine)
    \item In ampiezza
\end{itemize}
Nella visita \textbf{pre-ordine} si parte visitando il nodo della radice per poi passare a visitare tutto il nodo sinistro, risalendo poi andando verso destra.\\\\
Nella visita \textbf{in-ordine} si parte a visitare dal ramo più in basso a sinistra risalendo per poi andare verso destra. \\\\
Nella visita \textbf{post-ordine} vengono prima visitati i nodi più in profondità partendo sempre da sinistra verso destra per poi risalire.\\\\
Nella visita per \textbf{ampiezza} si analizza l'albero a livelli, partendo dalla radice.

\subsection{Alberi AVL}
Un albero $AVL$ è un albero di ricerca (quasi) bilanciato. Questo albero supporta le operazioni di $insert(), delete(), search()$ con costo $O(log n)$ nel \textbf{caso pessimo}.

\subsubsection{Fattore di bilanciamento}
Il fattore di bilanciamento $\beta(v)$ di un novo $v$ è dato dalla differenza tra l'altezza del sottoalbero sinistro e del sottoalbero destro di v:
$$\beta(v) = altezza(v.left)-altezza(v.right)$$
\subsubsection{Bilanciamento in altezza}
Un albero si dice \textbf{bilanciato in altezza} se le altezze dei sottoalberi sinitro e destro di ogni nodo differiscono al più di uno.
$$\beta\leq1$$
\textbf{Definizione:} un albero $AVL$ è un $ABR$ bilanciato in altezza.
\subsubsection{Inserimento e rimozione} 
Inserimenti e rimozioni richiedono di essere modificati per mantenere il bilanciamento dell'albero. \\
L'operazione fondamentale per ribilanciare l'albero è la \textbf{rotazione semplice}.\\
\subsubsection{Rotazione a sinistra}
Per effettuare questa rotazione prendo il nodo problematico e effettuo una rotazione scambiandolo con il successivo ed il nodo scambiato diventerà figlio destro mentre il figlio del nodo precendete diventerà figlio sinistro.

\subsection{Alberi 2-3}
Un albero 2-3 è un albero in cui:
\begin{itemize}
    \item Tutti i percorsi radice-foglia hanno la stessa lunghezza
    \item Le foglie contengono le chiavi (e i dati da memorizzare) e sono ordinate da sinistra verso destra in ordine di chiave crescente.
    \item Ogni nodo interno (non folgia) v ha 2 o 3 figli e mantiene due informazioni \begin{itemize}
        \item S[v], \textbf{chiave massima} nel sottoalbero sinitro (2 o 3 figli)
        \item M[v], \textbf{chiave massima} nel sottoalbero centrale (3 figli)
    \end{itemize}
    \item Distribuzione dei valori $k$ delle chiavi nei sottoalberi: \begin{itemize}
        \item Sinistro $k \leq S[v]$
        \item Centro $S[v] < k \leq M[v]$
        \item Destro $k > M[v]$
    \end{itemize}
\end{itemize}


\subsection{Tabelle Hash}
Le \textbf{tabelle hash} hanno una implementazione basata su una chiave $k$ e array.\\
Per ottenere la chiave sono presenti diverse tecniche di calcolo.\\
Ricapitolando, per realizzare una tabella hash efficiente abbiamo bisogno di:
\begin{itemize}
    \item Un vettore
    \item Una funzione has calcolabile velocemente e che garantisca una buona distribuzione delle chiavi nel vettore
    \item Un meccanisco per gestire le collisioni 
\end{itemize}
\subsubsection{Problema delle collisioni}
Una funzione hash $h$ si dice \textbf{perfetta} se è iniettiva:
$$\forall u, \in U: u \neq v \xrightarrow{} h(u) \neq h(v)$$
Se le collisioni sono inevitabili, è necessario trovare un metodo che le minimizzi, distribuendo \textbf{uniformemente} le chaivi negli indici della tabella hash.
\subsubsection{Funzioni hash}
E' necessario fare una premessa; nelle funzioni hash è \textbf{sempre possibile} trasformare una chiave complessa in un numero, (conversione in binario).
\subsubsection{Metodo dell'estrazione}
Le caratteristiche di questo metodo sono:
\begin{itemize}
    \item Usa solo una parte della chiave
    \item Si seleziona una sottosequenza di $p$ bit, con $m = 2^p$
    \item Solitamente dalle posizioni centrali
\end{itemize}
\textbf{esempio:} Verranno prese le cifre centrali 101000
\begin{center}
 bin(“beer”) = 000010 000101 000101 010010 
\end{center}
\begin{itemize}
    \item \textbf{Vantaggi:} molto veloce da calcolare
    \item \textbf{Svantaggi} rischio collisioni più alto di altri metodi
\end{itemize}

\subsubsection{Metodo della divisione}
Basata sul resto della divisione per $m$:
\begin{itemize}
    \item \textbf{Vantaggio:} molto veloce
    \item \textbf{Svantaggio:} Suscettibile a speficifi valori di $m$. Per risolvere questo problema bisogna scegliere $m$ come numero primo non troppo vicino ad una potenza di 2.
\end{itemize}
\textbf{Esempio:}
$$m=12, k=100 \xrightarrow{} h(k)=4$$
\subsubsection{Metodo della moltiplicazione}
Basato sulla moltiplicazione e il resto del numero
\begin{enumerate}
    \item Sia $A$ una costante, $0 < A < 1$
    \item Moltiplichiamo $k$ per $A$ e prendiamo la parte frazionaria
    \item Moltiplichiamo quest'ultima per $m$ e prendiamo la parte intera
\end{enumerate}
\textbf{Esempi:}
$$m = , k = 3, A = 0.8 \xrightarrow{} h(k) = 2$$
$$m = 1000,k = 123, A \approx 0.6180339887... \xrightarrow{} h(k) = 18$$
\begin{itemize}
    \item \textbf{Svantaggi:} lento (più lento del metodo di divisione)
    \item \textbf{Vantaggi} Il valore di $m$ non è critico
    \item \textbf{Come scegliere A?} $A \approx (\sqrt{5}-1)/2 = 0.61803...$ \textbf{(Knuth)}
\end{itemize}
\subsubsection{Metodo della codifica algebrica}
Metodo utilizzando dal compilatore java basato su espressioni algebriche:
$$h(k) = (k_n x^n + k_n_-_1 x^n^-^1 + ... + k_1 x + k_0) mod \hspace{0.1cm}m$$
$$k = k_n k_n_-_1 ... k_1k_0$$
Dove $k_0, k_1...$ possono essere, ad esempio, i bit della \textbf{codifica binaria} di $k$, oppure i \textbf{codici ascii} dei singoli caratteri di $k$.\\ $x$ è un valore \textbf{costante}.
\begin{itemize}
    \item \textbf{Vantaggi:} dipende da tutti i bit/caratteri della chiave
    \item \textbf{Svantaggi:} $n$ addizioni e $n*(n-1)/2$ prodotti
\end{itemize}
\subsubsection{Problema delle collisioni}
Attraverso questi metodi elencati precedentemente siamo riusciti a ridurre il numero di collisioni, ma senza eliminarle.\\\\
Per risolvere questo problema la  \textbf{complessita computazionale potrebbe aumentare a $n$}, possono essere utilizzate le seguenti tecniche:
\begin{enumerate}
    \item \textbf{Concatenamento}
    \item \textbf{Indirizzamento aperto}
\end{enumerate}
\subsubsection{concatenamento}
     Nella tecnica di  \textbf{concatenameto} gli elementi con lo stesso valore hash  $h$ vengono memorizzati in una lista concatenata (linked list).\\

Il \textbf{fattore di carico} è dato dal rapporto tra numero di elementi memorizzati e dimensioni della tabella. \\\\
La \textbf{complessità} del concatenamento è la seguente:
\begin{itemize}
    \item insert: $\Theta(1)$
    \item search: $\Theta(n)$
    \item delete: $\Theta(n)$
\end{itemize}
\subsubsection{indirizzamento aperto}
L'idea è quella di memorizzare tutte le chiavi nella tabella stessa, ed ogni slot contiene una chiave oppure $null$. \\\\
\textbf{Inserimento:} se lo slot prescelto è utilizzato, si cerca uno slot alternativo. \\\\
\textbf{Ricerca:} si cerca nello slot prescelto, epoi negli slot alternativi fino a quando non si trova la chiave oppure $null$.\\\\ 
Vengono utilizzati diversi algoritmi di indirizzamento, per esempio i seguenti:\\\\
\textbf{Ispezione lineare}\\
Il primo elemento determina l'intera sequenza. In questo modo si ottengono \textbf{lunghe sottosequenze}.
$$h(k,i) = (h' (k) + 1)$$
\\
\textbf{Ispezione quadratica:}\\
L'ispezione iniziale è in $h'(k)$, mentre le succesive hanno un offset che dipende da una \textbf{funzione quadratica nel numero di ispezione}.
$$h'(k) + c_1i + c_2 i^2$$
\\
\textbf{Doppio hashing:}\\
Formato da \textbf{due funzioni ausiliari} di cui la prima $h_1$ fornisce la prima ispezione, mentre $h_2$ fornisce l'offset delle succesive ispezioni.
$$h(k,i) = (h_1(k) + i h_2(k))$$

\subsubsection{Conclusioni hash table}
Usare funzioni hash $h(k)$ che producano valori il più possibile uniformemente distribuiti è molto importante perchè altrimenti potremmo arrivare ad una complessità computazionale pari a $O(n)$.\\\\
\textbf{Problemi con hashing:}
\begin{itemize}
    \item Scarsa locality of reference (cache miss)
    \item In base all'implementazione è in genere difficile ottenere le chiavi in ordine
    \item Sebbene il costo medio per operazione sia basso, la singola operazione può risultare molto costosa, ad esempio se occorre ridimensionare la tabell ae redistribuire le chiavi.
\end{itemize}
\newpage
\section{Scelta degli algoritmi}
A seconda delle operazioni da eseguire è necessario adattare diverse tecniche di implimentazione di un algoritmo.
\subsubsection{Implementazione su un vettore ordinato} 
Questo tipo di ricerca ha un costo computazionale basso nel caso in cui volessimo \textbf{ricercare degli elementi}.
Costi computazionali:
\begin{itemize}
    \item Ricerca $O(log\hspace{0.1cm}n)$
    \item Inserimento $O(n)$
    \item Eliminazione $O(n)$
\end{itemize}
\subsubsection{Implementazione su liste concatenate non ordinate}
Questo implementazione converrebbe utilizzarla nel caso in cui volessimo \textbf{aggiungere o eliminare degli elementi}.
Costi computazionali:
\begin{itemize}
    \item Ricerca $O(n)$
    \item Inserimento $O(1)$
    \item Eliminazione $O(n)$
\end{itemize}

\subsubsection{Implementazione alberi ABR}
Implementazione basata su alberi binari.
Costi computazionali:
\begin{itemize}
    \item Ricerca $O(h)$
    \item Inserimento $O(h)$
    \item Eliminazione $O(h)$
\end{itemize}

\subsubsection{Implementazione alberi AVL}
Implementazione basata su alberi binari a altezza equivalente.
Costi computazionali:
\begin{itemize}
    \item Ricerca $O(log \hspace{0.1cm}n)$
    \item Inserimento $O(log \hspace{0.1cm}n)$
    \item Eliminazione $O(log \hspace{0.1cm}n)$
\end{itemize}

\subsubsection{Implementazione alberi 2-3}
Implementazione basata su alberi binari ordinati con chiavi massime.
Costi computazionali:
\begin{itemize}
    \item Ricerca $O(log \hspace{0.1cm}n)$
    \item Inserimento $O(log \hspace{0.1cm}n)$
    \item Eliminazione $O(log \hspace{0.1cm}n)$
\end{itemize}

\subsubsection{Hash table}
Implementazione basata su array, dove l'elemento con chiave $k$ è memorizzato nel $k-esimo$ "slot" dell'array.

\begin{itemize}
    \item Ricerca caso medio $O(1)$ \hspace{2cm} Ricerca caso pessimo $O(n)$
    \item Inserimento caso medio $O(1)$ \hspace{1.2cm} Inserimento caso pessimo $O(n)$
    \item Eliminazione caso medio $O(1)$
    \hspace{1cm} Eliminazione caso pessimo $O(n)$
\end{itemize}

\subsection{Riepilogo}
\begin{center}
\includegraphics[width=12cm]{images/complessita.png}
\end{center}


\section{Algoritmi di ordinamento}
\subsubsection{ordinamento in loco:}
L'algoritmo permuta gli elementi direttamente nell'array
originale, senza usare un altro array di appoggio.
\subsubsection{ordinamento stabile:}
L'algoritmo preserva l'ordine con cui elementi con la stessa
chiave compaiono nell'array originale.
\subsection{Selection sort}
Cerca il minimo in $A[k+1..n]$ e spostalo in posizione $k+1$.\\
La complessita di questo algoritmo è pari a:
$$O(n^2)$$
\begin{center}
\includegraphics[width=5cm]{images/selectionSort.png}
\end{center}
\subsection{Insertion sort}
Inserisco l'elemento di posizione k+1 nella \textbf{posizione corretta} all'interno dei primi k elementi ordinati. Al termine del passo k, il vettore ha le prime k componenti  ordinate.\\
Il costo computazionale di questo algoritmo è:
$$O(n^2)$$
\begin{center}
\includegraphics[width=5cm]{images/insertionSort.png}
\end{center}
\newpage
\subsection{Bubble sort}
Ad ogni scansione scambia le coppie di elementi adiacenti che non sono nell'ordine corretto.\\
Ad ogni scansione \textbf{scambia le coppie di elementi adiacenti}.
Dopo la prima scansione, l'elemento massimo occupa l'ultima posizione, dopo la k-esima scansione, i k elementi massimi
occupano la posizione corretta in fondo all'array.
Nel caso $pessimo--ottimo$ bubble Sort ha costo:
$$\Theta(n^2) \hspace{1cm} \Theta(n)$$
\begin{center}
\includegraphics[width=7cm]{images/bubbleSort1.png}
\includegraphics[width=7cm]{images/bubbleSort2.png}
\includegraphics[width=7cm]{images/bubbleSort3.png}
\end{center}
\newpage
\subsection{Quick sort}
Scegli un elemento x del vettore v, e \textbf{partiziona il vettore in due parti} considerando gli elementi $\leq x$ e quelli $>x$\\
Ordina ricorsivamente le due parti.\\
Restituisci il risultato concatenando le due parti ordinate.
\subsubsection{Partizionamento}
Manteniamo due indici, inf e sup, che vengono fatti \textbf{scorrere dalle estremità} del vettore verso il centro. Quando entrambi (inf e sup) non possono essere fatti avanzare verso il centro, si \textbf{scambia} $A[inf]$ e $A[sup]$.\\
Il costo quick sort: Dipende dal partizionamento: 
\begin{center}
\textbf{caso peggiore:} $\Theta(n^2)$ \hspace{2cm} \textbf{caso migliore:} $\Theta(n \hspace{0.1cm}log \hspace{0.1cm}n)$
\end{center}
\begin{center}
\includegraphics[width=7cm]{images/quickSort.png}
\end{center}

\newpage

\subsection{Merge Sort}

Questo algoritmo \textbf{divide} $A[]$ in \textbf{due meta'} $A1[]$ e $A2[]$(senza permutare) di dimensioni uguali;\\
Applica \textbf{ricorsivamente} Merge Sort a
$A1[]$ e $A2[]$.\\
\textbf{Fonde} (merge) gli array ordinati $A1[]$ e
$A2[]$ per ottenere l'array $A[]$ ordinato.
\begin{center}
\includegraphics[width=9cm]{images/mergeSort.png}
\end{center}

\subsection{Heapsort}
Funzionamento: 
\begin{enumerate}
    \item Costruire un \textbf{max-heap} a partire dal vettore A[] originale, mediante l'operazione \textbf{heapify()}
    \item Estrarre il \textbf{massimo} $( findMax() + deleteMax() )$
    \item \textbf{Inserire} il massimo in ultima posizione di A[].
    \item \textbf{Ripetere} il punto 2. finché lo heap diventa vuoto
\end{enumerate}
\newpage
\subsubsection{Albero binario perfetto}
Un albero binario è \textbf{perfetto} se:


\begin{itemize}
    \item Tutte le foglie hanno la stessa altezza h
    \item Nodi interni hanno grado 2
\end{itemize}
 Un albero perfetto ha altezza $h\hspace{0.1cm} \simeq log \hspace{0.1cm} N$  \\
 Il numero di nodi è $ N = nodi = 2^h^+^1 -1$
 
 \subsubsection{Albero binario completo}
 Un albero binario è \textbf{completo} se:
\begin{itemize}
    \item Tutte le foglie hanno profondità h o h-1
    \item Tutti i nodi a livello h sono “accatastati” a
    sinistra
    \item Tutti i nodi interni hanno grado 2, eccetto al più uno
\end{itemize}
\subsubsection{Max-heap}
Un albero binario completo è un albero \textbf{max-heap} sse:
\begin{itemize}
    \item Ad ogni nodo i viene associato un valore $A[i]$
    \item $A[Parent(i)] \geq A[i]$
\end{itemize}
\subsubsection{Min-heap}
Un albero binario completo è un albero \textbf{min-heap} sse:
\begin{itemize}
    \item Ad ogni nodo i viene associato un valore $A[i]$
    \item $A[Parent(i)] \leq A[i]$
\end{itemize}
\newpage

\subsection{Operazioni su array heap}
\subsubsection{findMax()} 
Individua il valore massimo contenuto in uno heap.\\
Il massimo è sempre la radice, ossia A[1].\\
L'operazione ha costo $\Theta(1)$.

\subsubsection{fixHeap()}
Ripristinare la proprietà di max-heap.\\
Supponiamo di rimpiazzare la radice $A[1]$ di un max-heap
con un valore qualsiasi, vogliamo fare in modo che $A[]$ diventi nuovamente uno heap.

\subsubsection{heapify()}
Costruire uno heap a partire da un array privo di alcun ordine.

\subsubsection{deleteMax()}
Rimuovi l'elemento massimo da un maxheap $A[]$.

\subsubsection{Esempio heapSort:}
\begin{center}
\includegraphics[width=10cm]{images/heapSort.png}
\end{center}

\includegraphics[width=16cm]{images/sommarioAlgoritmi.png}


\section{Tecniche lineari di ordinamento}
\subsection{Counting Sort}
I valori di $A[0..n-1]$ appartengono all'intervallo $[0, k-1]$
(ciascun valore può comparire zero o più volte).\\
Costruisco un array $Y[0, k-1]$; $Y[i]$ conta il numero di volte in
cui il valore i compare in $A[]$.\\
Ricolloco i valori così ottenuti in A.\\
\textbf{Counting Sort: Costo}
$$O(max\{n,k\}) = O(n+k) = O(n)$$

\begin{center}
\includegraphics[width=5cm]{images/countingSort.png}
\end{center}



\subsection{Bucket Sort}
Cosa succede se i valori da ordinare non sono numeri interi,
ma record associati ad una chiave?\\
Possiamo usare liste concatenate.
\begin{center}
\includegraphics[width=12cm]{images/bucketSort.png}
\end{center}
\\
\textbf{Bucket Sort: Costo}
$$O(n+k)$$

\subsection{Radix Sort}
Supponiamo di voler ordinare n numeri con 4 cifre decimali.\\
Questo richiederebbe $n+10000$ operazioni; se $n \hspace{0.1cm} log \hspace{0.1cm} n < n+10000$, questo non sarebbe conveniente.\\
Prima ordino in base alla cifra delle \textbf{unità}.\\
Poi ordino in base alla cifra delle \textbf{decine}.\\
Poi ordino in base alla cifra delle \textbf{centinaia}.\\

\begin{center}
\includegraphics[width=12cm]{images/radixSort.png}
\end{center}




\section{Selezione del k-esimo}
Consideriamo il seguente problema:\\
\textbf{Selezione del k-esimo minimo}: dato un array $A[1..n]$ di valori
distinti e un valore $1 \leq k \leq n$, trovare l'elemento che è
maggiore di esattamente $k-1$ elementi.\\
\textbf{Mediano}: il valore che occuperebbe la posizione $(n/2)$ se
l'array fosse ordinato.\\
I \textbf{motori di ricerca} producono molti risultati a fronte di
una singola query. I risultati vengono mostrati in pagine, in ordine
decrescente di rilevanza. È \textbf{inutile ordinare tutti i risultati} in base alla rilevanza.\\
Verifichiamo ora i costi computazionali dei singoli casi:\\
\textbf{Ricerca del minimo:}\\
$$T(n) = n-1 = \Theta(n)$$
\textbf{Ricerca del secondo minimo:}\\
$$T(n) = 2n-3 = \Theta(n)$$
\textbf{Selezione del k-esimo elemento:}\\
$$T(n) = \Theta(kn)$$
\textbf{Selezione del valore mediano:}\\
$$T(n) = O(n + k\hspace{0.1cm} log\hspace{0.1cm} n) = O(n + (n/2) log \hspace{0.1cm} n) = O(n \hspace{0.1cm} log \hspace{0.1cm} n)$$
\subsubsection{Adattamento di quicksort al
problema della selezione:}
In questo modo divido il mio array in più partizioni, andando a eliminare quelle inutili.
\begin{center}
\includegraphics[width=8cm]{images/partizionamento1.png}
\includegraphics[width=8cm]{images/partizionamento2.png}
\end{center}
\subsubsection{Analisi dell'algoritmo quickSelect()}
\textbf{Costo nel caso ottimo:}\\
$$T(n) = T(n/2) + n = \Theta(n)$$
\textbf{Costo nel caso pessimo:}\\
$$T(n) = T(n-1) + n = \Theta(n^2)$$
\textbf{Costo nel caso medio:}\\
$$T(n) \leq 4n$$

\section{Code con priorità}
Le code con priorità sono strutture dati che mantengono il minimo (massimo) in un insieme dinamico di chiavi. 
$$ coda = key | elem $$
Sono presenti due possibili implementazioni:
\begin{itemize}
    \item d-heap
    \item heap binomiali e heap di fibonacci
\end{itemize}
\subsubsection{d-heap}
Un d-heap è un albero d-ario con le seguenti proprietà:
\begin{enumerate}
    \item Un d-heap di altezza $h$ è perfetto almeno fino alla profondità $h-1$; le foglie al livello $h$ sono accatastate a sinistra.
    \item Ciascun nodo $v$ contiene una $chiave(v)$ e un elemento $elem(v)$. Le chiavi appartengono ad un dominio totalmente ordinato.
    \item Ogni nodo diverso dalla radice ha chiave non inferiore $(\geq)$ a quella del padre.
\end{enumerate}
\newpage
\textbf{Esempio d-heap:} $d = 3$
\begin{center}
\includegraphics[width=12cm]{images/d-heap.png}
\end{center}
Un d-heap con n nodi ha \textbf{altezza} $O(log_d \hspace{0.1cm} n)$
\begin{center}
\includegraphics[width=12cm]{images/costiD-heap.png}
\end{center}

\newpage

\section{Union find}
L'\textbf{union find} è una struttura dati basata sugli insiemi.\\
Essa può creare un insieme a partire da un singolo elemento, unire due insiemi, identificare l'insieme a cui appartiene un elemento. Gli insiemi contengono complessivamente $n \leq k$ elementi. Ogni insieme è identificato da un rappresentante univoco.\\
\textbf{Operazioni Union find:}

\begin{itemize}
    \item $makeSet(elem \hspace{0.1cm} x)$:  Crea un insieme il cui unico elemento (e rappresentante) è $x$. Esso non deve appartenere ad un altro insieme esistente.
    \item $find(elem \hspace{0.1cm} x) \xrightarrow{} name$: Restituisce il rappresentante dell’unico insieme contenente $x$.
    \item $union(name \hspace{0.1cm} x, name \hspace{0.1cm} y)$: Unisce i due insiemi rappresentati da $x$ e da $y$. Assumiamo che il nome del nuovo insieme sia $x$. I vecchi insiemi devono essere distrutti. 
\end{itemize}
\textbf{Esempio:}
\begin{center}
\includegraphics[width=12cm]{images/unionFind.png}
\end{center}

\newpage

\subsection{QuickFind}
Ogni insieme viene rappresentato con un \textbf{albero} di altezza uno.
\begin{itemize}
    \item Le foglie dell'albero contengono gli elementi dell'insieme.
    \item Il rappresentante è la radice.
\end{itemize}
\begin{center}
\includegraphics[width=12cm]{images/operazioniUnionFind.png}
\end{center}

\newpage

\subsection{QuickUnion}
Implementazione basata su foresta.
\begin{itemize}
    \item Si rappresenta ogni insieme tramite un albero radicato generico.
    \item Ogni nodo dell'albero contiene l'oggetto e il puntatore al padre.
    \item Il rappresentante è la radice.
\end{itemize}

\begin{center}
\includegraphics[width=12cm]{images/quickUnion.png}
\end{center}

\begin{center}
\includegraphics[width=12cm]{images/riepilogoUnionFind.png}
\end{center}

E' conveniente utilizzare \textbf{QuickFind} quando le $union()$ sono rare e le $find()$ frequenti.\\
E' conveniente utilizzare \textbf{QuickUnion} quando le $find()$ sono rare e le $union()$ frequenti.\\

\newpage

\section{Divide et Impera}
\begin{itemize}
    \item Divide-et-impera \begin{itemize}
        \item Un problema viene suddiviso in sotto-problemi, che vengono risolti ricorsivamente (top-down).
    \end{itemize}
    \item Algoritmi greedy \begin{itemize}
        \item Ad ogni passo si fa sempre la scelta che in quel momento appare ottima; le scelte fatte non vengono mai disfatte
    \end{itemize}
    \item Programmazione dinamica \begin{itemize}
        \item La soluzione viene costruita (bottom-up) a partire da un insieme di sotto-problemi
    \end{itemize}
\end{itemize}

Tecnica divisa in \textbf{3 fasi} fondamentali:
\begin{enumerate}
    \item \textbf{Divide:} Dividi il problema in sotto-problemi indipendenti, di dimensioni $minori$.
    \item  \textbf{Impera:} Risolvi i sotto-problemi ricorsivamente.
    \item \textbf{Combina:} Unisci le soluzioni dei sottoproblemi per costruire la soluzione del problema di partenza
\end{enumerate}

\subsection{Algoritmi greedy}
Quando applicare la tecnica greedy?
\begin{itemize}
    \item Quando è possibile dimostrare che esiste una \textbf{scelta ingorda}.
    \begin{itemize}
        \item Fra le molte scelte possibili, se ne può facilmente individuare una che porta sicuramente alla soluzione ottima.
    \end{itemize}
    \item Quando il problema ha \textbf{sottostruttura ottima}.
    \begin{itemize}
        \item “Fatta tale scelta, resta un sottoproblema con la stessa
        struttura del problema principale”.
    \end{itemize}   
\end{itemize}

\subsection{Algoritmo di scheduling}
Algoritmo che si basa in base al tempo medio di eseguzione:
\begin{itemize}
    \item 1 processore, n job $p_1$ , $p_2$, ..., p.
    \item Ogni job $p_i$ ha un tempo di esecuzione $t[i]$.
    \item Minimizzare il tempo medio di completamento.
\end{itemize}

\begin{center}
\includegraphics[width=12cm]{images/scheduling.png}
\end{center}


\subsection{Codifica di Huffman}

Questa codifica viene utilizzata per risolvere il problema di compressione, (compressione di un file). Viene utilizzata una tecnica detta \textbf{codifica di caratteri}
\begin{itemize}
    \item Si usa una \textbf{funzione di codifica f: $f(c) = x$}
    \begin{enumerate}
        \item $c$ è un carattere preso da un alfabeto $\sum$
        \item $x$ è una rappresentazione binaria del carattere $c$
        \item $c$ è rappresenrtato da $x$ in modo efficiente
    \end{enumerate}
    \item Una sequenza di caratteri $c_1 c_2 c_n$ viene codificato con la sequenza di bit $f(c_1)f(c_2)f(c_n)$
    \item data una qualsiasi codifica, deve essere sempre possibile decodificarla durante la lettura seuqenziale bit-dopo-bit.
\end{itemize}
Dobbiamo utilizzare una codifica che minimizza la dimensione del nostro file.

\subsubsection{Codifica a lunghezza fissa}
\begin{center}
\includegraphics[width=12cm]{images/fissa.png}
\end{center}
\begin{itemize}
    \item Supponiamo di avere un file di $n$ caratteri.
    \item Codifica tramite ASCII (8 bit per carattere).
    \item Codifica basata sull'alfabeto ( 3 bit per carattere).
\end{itemize}

\subsubsection{Codifica a lunghezza variabile}
\begin{center}
\includegraphics[width=12cm]{images/variabile.png}
\end{center}
\begin{itemize}
    \item Codifica a lunghezza variabile
    \item \textbf{codice a prefisso} (senza prefissi):
    \begin{itemize}
        \item \textbf{nessun codice è un prefisso di un altro codice}.
        \item Condizione richiesta per permettere sempre la decodifica durante la lettura bit-dopo-bit.
    \end{itemize}
\end{itemize}
In questo modo avendo dei \textbf{prefissi univoci}, non appena troviamo una sequenza di bit possiamo risalire alla decodifica della parola. In questo modo \textbf{lettere frequenti} saranno composte da \textbf{pochi bit}.

\newpage

\subsubsection{Codici di Huffman}
Rappresentazione del codice come un albero binario
\begin{itemize}
    \item Figlio sinistro: $0$ \hspace{1cm} Figlio destro: $1$
    \item Caratteri dell'alfabeto sulle foglie
\end{itemize}

\begin{center}
\includegraphics[width=13cm]{images/codificaAlbero.png}
\end{center}
Questo esempio per essere ottimizzato deve avere anche un figlio destro al primo livello di profondità.
\begin{center}
\includegraphics[width=10cm]{images/codificaAlbero2.png}
\end{center}
Il \textbf{principio del codice di Huffman} è:
\begin{itemize}
    \item \textbf{Minimizzare la lunghezza dei caratteri} che compaiono più frequentemente.
    \item Assegnare ai caratteri con la frequenza i codici corrispondenti ai percorsi più lunghi all'interno dell'albero.
\end{itemize}
\begin{enumerate}
    \item Inizialmente costruiamo una lista ordinata di nodi, in cui ogni nodo continee un carattere e il numero di volte in cui quel carattere compare nel file.
    $$ "f":5 \hspace{0.5cm} "e":9 \hspace{0.5cm} "c":12 \hspace{0.5cm} "b":13 \hspace{0.5cm} "d":16 \hspace{0.5cm} "a":45 $$
    \item Rimuovere i due nodi con frequenze minori.
    \item Collegarli ad un nodo padre etichettato con la frequenza combinata (sommata).
    $$"c":12 \hspace{0.5cm} "b":13 \hspace{0.5cm} "d":16 \hspace{0.5cm} "a":45 $$
    \item Aggiungere il nodo combinato alla lista, mantenendola ordinata in base alla frequenza.
\end{enumerate}
\begin{center}
\includegraphics[width=8cm]{images/huffmanFinale.png}
\end{center}
\textbf{Vantaggi:}
\begin{itemize}
    \item Semplici da programmare
    \item Solitamente efficienti
    \item Quando è possibile dimostrare la proprietà di scelta greedy
        danno la soluzione ottima
    \item  La soluzione sub-ottima può essere accettabile
\end{itemize}

\textbf{Svantaggi:}
\begin{itemize}
    \item Non tutti i problemi ammettono una soluzione greedy
    \item Quindi, in certi casi gli algoritmi greedy non possono essere
          usati se si vuole la soluzione ottima 
\end{itemize}

\section{Programmazione dinamica}
\begin{center}
\includegraphics[width=12cm]{images/DivideDinamica.png}
\end{center}
\begin{enumerate}
    \item \textbf{Sottostruttura ottimale}, deve essere possibile combinare le soluzioni dei sottoproblemi. 
    \item \textbf{Sottoproblemi ripetuti}, che ricompaiono constantemente.
\end{enumerate}

\newpage
\subsection{Distanza di Levenshtein}

Tecnica utilizzata dai correttori ortografici. Basata su:
\begin{itemize}
    \item Concetto di edit distance:
    \begin{itemize}
        \item Numero di operazioni di “editing” che sono necessarie per
        trasformare una stringa S in una nuova stringa T.
    \end{itemize}
    \item Transformazioni ammesse:
    \begin{itemize}
        \item Lasciare immutato il carattere corrente (costo 0).
        \item Cancellare un carattere (costo 1).
        \item Inserire un carattere (costo 1).
        \item Sostituire il carattere corrente con uno diverso (costo 1).
    \end{itemize}
    \item Dopo ciascuna operazione ci si sposta sul carattere
    successivo:
    \begin{itemize}
        \item Si inizia dal primo carattere di $S$.
    \end{itemize}
\end{itemize}

La \textbf{distanza di levenshtein} tra $S[1..n]$ e $T[1..m]$ è il \textbf{costo minimo} tra tutte le sequenze di operazioni di editing che trasformano $S$ in $T$.\\\\
\textbf{Esempio:}\\
- Determinare il numero minimo di operazioni di editing necessarie per trasformare il prefisso $S[1..i]$ di $S$ nel prefisso $T[1..j$ di $T$.\\
- La definizione della soluzione è data da $L[1..j]$.
- La distanza di Levenshtein tra $S[1..n]$ e$T[1..m]$ è il valore $L[n,m]$.
  
\newpage
  
\section{Grafi}

\subsection{Grafici orientati e non orientati}
\begin{itemize}
    \item Un \textbf{Grafico orientato G} è una coppia $(V,E)$ dove:
    \begin{itemize}
        \item Insieme finito dei \textbf{vertici V}
        \item Insieme degli \textbf{archi E}: relazione binaria tra vertici
    \end{itemize}
\end{itemize}
\begin{center}
\includegraphics[width=8cm]{images/grafoOrientato.png}
\end{center}

\begin{itemize}
    \item Un \textbf{grafo non orientato G} è una coppia $(V,E)$ dove:
    \begin{itemize}
        \item Insieme finito dei \textbf{vertici V}
        \item Insieme degli \textbf{archi E}: coppie non ordinate
    \end{itemize}
\end{itemize}
\begin{center}
\includegraphics[width=8cm]{images/grafoNonOrientato.png}
\end{center}

\subsection{Problemi sui grafi}

\begin{itemize}
    \item Visite
    \begin{itemize}
        \item Visite in ampiezza
        \item Visite in profondità
    \end{itemize}
    \item Alberi di copertura minimi
    \item Cammini minimi
    \begin{itemize}
        \item Da singola sorgente
        \item Fra tutte le coppie dei vertici 
    \end{itemize}
\end{itemize}

\subsubsection{Incidenza e adiacenza}
\begin{itemize}
    \item In un grafo orientato l'arco \textbf{(v,w)} è \textbf{incidente} da $v$ a $w$
    \item Un vertice $w$ è adiacente a $v$ se e solo se $(v, w) \in E$
    \item In un grafo non orientato la relazione di adiacenza tra
     vertici è simmetrica
\end{itemize}
\begin{center}
\includegraphics[width=10cm]{images/grafoincidente.png}
\end{center}

\newpage

\begin{center}
\includegraphics[width=8cm]{images/interfaceGrafi.png}
\end{center}


$n$ = \textbf{vertici}\\
$m$ = \textbf{numero archi}
\subsection{Rappresentazioni di grafi}
\begin{itemize}
    \item Liste di archi:
    \begin{itemize}
        \item  grado = $O(m)$
        \item  archiIncidenti = $O(m)$
        \item  sonoAdiacenti = $O(m)$
        \item  aggiungiVertice = $O(1)$
        \item  aggiungiArco = $O(1)$
        \item  rimuoviVertice = $O(m)$
        \item  rimuoviArco = $O(1)$
    \end{itemize}
\end{itemize}

\begin{center}
\includegraphics[width=7cm]{images/listeArchi.png}
\end{center}
$\delta$ = grado
\begin{itemize}
    \item Liste di incidenza
    \begin{itemize}
        \item  grado = $O(\delta(n))$
        \item  archiIncidenti = $O(\delta(n))$
        \item  sonoAdiacenti = $O(min{\delta(x), \delta(y)})$
        \item  aggiungiVertice = $O(1)$
        \item  aggiungiArco = $O(1)$
        \item  rimuoviVertice = $O(m)$
        \item  rimuoviArco = $O(\delta(x)+\delta(y))$
    \end{itemize}
\end{itemize}


\begin{center}
\includegraphics[width=10cm]{images/listeIncidenza.png}
\end{center}


\begin{itemize}
    \item Matrice di adiacenza
    \begin{itemize}
        \item  grado = $O(n)$
        \item  archiIncidenti = $O(n)$
        \item  sonoAdiacenti = $O(1)$
        \item  aggiungiVertice = $O(n^2)$
        \item  aggiungiArco = $O(1)$
        \item  rimuoviVertice = $O(n^2)$
        \item  rimuoviArco = $O(1)$
    \end{itemize}
\end{itemize}

\begin{center}
\includegraphics[width=12cm]{images/listeMatrice.png}
\end{center}


\begin{itemize}
    \item Liste di adiacenza
    \begin{itemize}
        \item  grado = $O(\delta(v))$
        \item  archiIncidenti = $O(\delta(v))$
        \item  sonoAdiacenti = $O(min{\delta(x), \delta(y)})$
        \item  aggiungiVertice = $O(1)$
        \item  aggiungiArco = $O(1)$
        \item  rimuoviVertice = $O(m)$
        \item  rimuoviArco = $O(\delta(x)+\delta(y))$
    \end{itemize}
\end{itemize}

\begin{center}
\includegraphics[width=12cm]{images/listeAdiacenza.png}
\end{center}

\subsection{Grafi pesati}
In alcuni casi ogni arco ha un \textbf{peso} (o \textbf{costo}) associato.Il costo può essere determinato tramite una funzione di costo c: $E \in R$, dove $R$ è l’insieme dei numeri reali. Quando tra due vertici non esiste un arco, si dice che il costo è \textbf{infinito}.

\subsection{Grado}
In un \textbf{grafo non orientato}, il \textbf{grado} di un vertice è il \textbf{numero di archi} che partono da esso.

\begin{center}
\includegraphics[width=10cm]{images/gradoGrafi.png}
\end{center}

\begin{itemize}
    \item In un \textbf{grafo orientato}, il grado entrante (uscente) di un
vertice è il \textbf{numero di archi incidenti} in (da) esso
\item In un \textbf{grafo orientato} il grado di un vertice è la \textbf{somma} del suo grado entrante e del suo grado uscente
\end{itemize}

\subsection{Cammini}
La lunghezza del cammino è il numero di archi attraversati.

\begin{center}
\includegraphics[width=10cm]{images/cammini.png}
\end{center}

Un cammino si dice \textbf{semplice} se tutti i suoi vertici sono
\textbf{distinti} (compaiono una sola volta nella sequenza).

\subsection{Grafi connessi}
Se G è un grafo \textbf{non orientato}  , diciamo che $G$ è
\textbf{connesso} se esiste un cammino da ogni vertice ad
ogni altro vertice. 

\begin{center}
\includegraphics[width=10cm]{images/grafoConnesso.png}
\end{center}
\subsubsection{Grafo fortemente connesso}
Se G è un grafo \textbf{orientato}, diciamo che $G$ è \textbf{fortemente
connesso} se esiste un cammino da ogni vertice ad
ogni altro vertice.

\begin{center}
\includegraphics[width=6cm]{images/grafoFortementeConnesso.png}
\end{center}

\subsubsection{Grafo debolmente connesso}

Se G è un grafo \textbf{orientato} che non è fortemente
connesso, ma la sua versione non orientata è
connessa, diciamo che G è \textbf{debolmente connesso}.

\begin{center}
\includegraphics[width=10cm]{images/grafoDebolmenteConnesso.png}
\end{center}

\subsection{Grafi aciclici}
Un grafo senza cicli semplici è detto \textbf{aciclico}.
Un grafo orientato aciclico è chiamato \textbf{DAG} (Directed Acyclic Graph).
\begin{center}
\includegraphics[width=7cm]{images/grafoAciclico.png}
\end{center}

\subsection{Grafo completo}
Un \textbf{grafo non orientato completo} è un grafo non
orientato che ha un arco tra ogni coppia di vertici.

\begin{center}
\includegraphics[width=8cm]{images/grafoCompleto.png}
\end{center}

\subsection{Alberi}

 Un \textbf{albero libero} è un grafo non orientato connesso,
aciclico. Se un vertice è detto radice, otteniamo un \textbf{albero
radicato}

\begin{center}
\includegraphics[width=8cm]{images/albero.png}
\end{center}

\section{Algoritmi di Visita di grafi}
\begin{itemize}
    \item \textbf{Visita in ampiezza} (breadth-first search)
    \begin{itemize}
        \item Visita i nodi “espandendo” la frontiera fra nodi scoperti / da
scoprire
\item Es: Cammini di lunghezza minima da singola sorgente
    \end{itemize}
    \item \textbf{Visita in profondità} (depth-first search)
    \begin{itemize}
        \item Visita i nodi andando il “più lontano possibile” nel grafo
        \item Es: Componenti fortemente connesse, ordinamento
topologico
    \end{itemize}
\end{itemize}

\subsubsection{Vertici del grafo}
Ogni vertice del grafo può essere:
\begin{itemize}
    \item \textbf{inesplorato:} Il vertice non è ancora stato incontrato.
    \item \textbf{aperto:} l'algoritmo ha incontrato il vertice la prima volta.
    \item \textbf{chiuso:} il vertice è stato visitato completamente (tutti gli archi incidenti sono stati esplorati).
\end{itemize}
\subsection{Algoritmo di visita generico}
\begin{center}
    \includegraphics[width=10cm]{images/visitaGrafi.png}
\end{center}
\subsubsection{Complessità}
\begin{itemize}
    \item $O(n+m)$ liste di adiacenza
    \item $O(n^2)$ matrice di adiacenza
\end{itemize}

\subsection{Algoritmo di visita in ampiezza}
\begin{itemize}
    \item Visitare i nodi a distanze crescenti dalla sorgente
    \item Generare un albero BF (breadth-first), cioè un albero contentente tutti i vertici
    \item Calcolare la distanza minima da s a tutti i vertici raggiungibili
\end{itemize}

\begin{center}
    \includegraphics[width=8cm]{images/visitaAmpiezza.png}
\end{center}

\subsection{Algoritmo di visita in profondità}
\begin{itemize}
    \item Utilizzata per coprire l'intero grafo, non solo i nodi
raggiungibili da una singola sorgente (diversamente da BFS)
\item Informazioni addizionali sul tempo di visita

\end{itemize}
\begin{center}
    \includegraphics[width=8cm]{images/visitaProfondita.png}
\end{center}

\subsubsection{Proprietà della visita DFS}

In una qualsiasi visita in profondità per ogni coppia di vertici $u,v$ \textbf{una sola} delle seguenti condizioni è vera:
\begin{enumerate}
    \item Gli intervalli $[u.dt, u.ft]$ e $[v.dt, v.ft]$ sono disgiunti (non sono discendenti)
    \item L'intervallo $[u.dt, u.ft]$ è interamente contenuto in $[v.dt, v.ft]$ ($u$ è discendente di $f$
    \item L'intervallo $[v.dt, v.ft]$ è interamente contenuto in $[u.dt, u.ft]$ ($v$ è discendente di $u$
\end{enumerate}


\subsection{Ordinamento topologico}
Dato un DAG G (direct acyclic graph), un ordinamento
topologico su G è un ordinamento lineare dei suoi
vertici tale per cui:
\begin{itemize}
    \item Se G contiene l’arco (u,v), allora u compare prima di v
nell’ordinamento
\item Per transitività, ne consegue che se v è raggiungibile da u,
allora u compare prima di v nell'ordinamento
\end{itemize}

\begin{center}
    \includegraphics[width=8cm]{images/ordinamentoTopologico.png}
\end{center}

\newpage

\subsubsection{Algoritmo per ordinamento topologico}

Algoritmo:
\begin{enumerate}
    \item Si effettua una DFS
    \item L’operazione di visita aggiunge il nodo alla testa di una lista “at finish time”
    \item Restituire la lista di vertici
\end{enumerate}

\begin{center}
    \includegraphics[width=5cm]{images/esempioOrdinamento.png}
\end{center}

\subsection{Collegare elementi minimizzando vincoli}
Minimizzare la quantità di filo elettrico per collegare fra loro i
diversi componenti.
\begin{itemize}
    \item albero di copertura (di peso) minimo.
    \item albero di connessione (di peso) minimo.
    \item minimum spanning tree.
\end{itemize}

\subsubsection{Albero di copertura (spanning tree)}
Dato un grafo $G = (V, E)$ non orientato e connesso, un
albero di copertura di $G$ è un sottografo $T = (V, E_T
)$ tale che:
\begin{itemize}
    \item  T è un albero
    \item $E_T \subseteq E$
    \item $T$ contiene tutti i nodi di $G$
\end{itemize}

\begin{center}
    \includegraphics[width=5cm]{images/alberoCopertura.png}
\end{center}
Il \textbf{Minimum Spanning Tree} non è necessariamente unico.

\subsection{Algoritmo generico}
 \begin{itemize}
 \item \textbf{Vediamo:}
     \begin{itemize}
         \item  Un algoritmo \textbf{greedy} generico.
         \item Due istanze di questo algoritmo: \textbf{Kruskal} e \textbf{Prim}
     \end{itemize}
     \item L'idea è di \textbf{accrescere} un sottoinsieme T di archi in
        modo tale che venga rispettata la seguente condizione:
        \begin{itemize}
            \item T è un sottoinsieme di qualche albero di copertura minimo
        \end{itemize}
        \item  Un arco ${u, v}$ è detto \textbf{sicuro} per T se $T \cup \{u, v\}$ è ancora un sottoinsieme di qualche MST
  \end{itemize}

\begin{center}
    \includegraphics[width=7cm]{images/algoritmoCopertura.png}
\end{center}
\begin{itemize}
    \item \textbf{Archi blu}
    \begin{itemize}
        \item sono gli archi che fanno parte del MST.
    \end{itemize}
    \item \textbf{Archi rossi}
    \begin{itemize}
        \item sono gli archi che non fanno parte del MST
    \end{itemize}
\end{itemize}

Per caratterizzare gli archi sicuri dobbiamo introdurre
alcune definizioni:
\begin{itemize}
    \item Un \textbf{taglio} $(S, V - S)$ di un grafo non orientato $G = (V, E)$ è una partizione di $V$ in due sottoinsiemi disgiunti
    \item Un arco ${u, v}$ \textbf{attraversa il taglio} se $u \in S e v \in V - S$
    \item Un taglio \textbf{rispetta} un insieme di archi T se nessun arco di T
    attraversa il taglio.
    \item Un arco che attraversa un taglio è \textbf{leggero} se il suo peso è minimo fra i pesi degli archi che attraversano un taglio.
\end{itemize}

\begin{enumerate}
    \item \textbf{Regola del taglio}
    \begin{itemize}
        \item Scegli un taglio in G che \textbf{non contenga archi blu}. Tra tutti gli archi non colorati che attraversano il taglio seleziona un arco di costo minimo e coloralo di blu
    \end{itemize}
    \item \textbf{Regola del ciclo}
    \begin{itemize}
        \item Scegli un ciclo semplice in $G$ che \textbf{non contenga archi rossi.} Tra tutti gli archi non colorati del ciclo, seleziona un arco di costo massimo e coloralo di rosso
    \end{itemize}
    \item \textbf{Metodo greedy}
    \begin{itemize}
        \item Costruisce un MST applicando, ad ogni passo, una delle
        due regole precedenti (una qualunque, purché si possa usare)
    \end{itemize}
\end{enumerate}

\begin{center}
    \includegraphics[width=7cm]{images/esempioCopertura.png}
\end{center}


\subsubsection{Algoritmo di Kruskal}
\begin{itemize}
    \item Ingrandire sottoinsiemi disgiunti di un albero di copertura
    minimo connettendoli fra di loro fino ad avere l’albero finale
    \begin{itemize}
        \item Inizialmente la \textbf{foresta di copertura} è composta da n alberi, uno per ciascun nodo, e nessun arco.
    \end{itemize}
    \item Si considerano gli archi in ordine non decrescente di peso
    \item L’algoritmo è greedy perché ad ogni passo si aggiunge alla
    foresta un arco con il peso minimo
\end{itemize}

\begin{center}
    \includegraphics[width=7cm]{images/kruskal.png}
\end{center}
\textbf{Costo computazionale:}\\
$n$ = \textbf{vertici}\\
$m$ = \textbf{numero archi}
$$O(m \hspace{0.1cm}log\hspace{0.1cm} n)$$

\subsection{Algoritmo di Prim}
L’algoritmo di \textbf{Prim} procede mantenendo in un singolo albero $T$ che viene fatto via via “crescere”.
\begin{itemize}
    \item L’albero parte da un nodo arbitrario \textbf{r} (la \textbf{radice}) e cresce fino a quando ricopre tutti i vertici.
    \item Ad ogni passo viene aggiunto l'arco di peso minimo che
    collega un nodo già raggiunto dell'albero con uno non ancora raggiunto
\end{itemize}

\begin{center}
    \includegraphics[width=7cm]{images/prim.png}
\end{center}
\textbf{Costo computazionale:}\\
$n$ = \textbf{vertici}\\
$m$ = \textbf{numero archi}
$$O(m \hspace{0.1cm}log\hspace{0.1cm} n)$$

\section{Cammini minimi}
Consideriamo un grafo orientato $G = (V,E)$ in cui ad ogni arco $(x, y) \in E$ sia associato un costo $w(x, y)$.\\
Data una coppia di nodi $v_0$ e $v_k$ , vogliamo trovare (se esiste) un cammino $\pi v_0_v_k*$  di costo minimo tra tutti i cammini che vanno da $v_0 a v_k$

\textbf{Problemi simili da risolvere:}\\
\begin{enumerate}
    \item \textbf{Cammino di costo minimo fra una singola coppia di nodi} $u$ e $v$
    \begin{itemize}
        \item Determinare, se esiste, un cammino di costo minimo $\pi_u_v*$ da $u$ verso $v$.
    \end{itemize}
    \item \textbf{Single-source shortest path}
    \begin{itemize}
        \item Determinare cammini di costo minimo da un nodo sorgente $s$ a tutti i nodi raggiungibili da $s$.
    \end{itemize}
    \item \textbf{All-pairs shortest paths}
    \begin{itemize}
        \item Determinare cammini di costo minimo tra ogni coppia di nodi $u, v$
    \end{itemize}
\end{enumerate}

\subsubsection{Proprietà (sottostruttura ottima)}
Sia $G = (V, E)$ un grafo orientato con funzione costo $w$; allora ogni sotto-cammino di un cammino di costo minimo in $G$ è a sua volta un cammino di costo minimo.

\subsection{Condizioni di Bellman Ford}
\begin{itemize}
    \item Per ogni arco $(u, v)$ e per ogni vertice $s$, vale la seguente disuguaglianza
    $$d_s_v \leq d_s_u+w(u,v) $$
\end{itemize}
Dalla condizione di \textbf{Bellman} si può dedurre che l'arco $(u, v)$ fa parte di un cammino di \textbf{costo minimo} $\pi_s_v*$ se e solo se:
    $$d_s_v = d_s_u+w(u,v) $$

\begin{itemize}
    \item Supponiamo di mantenere una stima $D_s_v \leq d_s_v$ della lunghezza del cammino di costo minimo tra $s$ e $v$.
    \item Effettuiamo dei passi di “rilassamento”, riducendo progressivamente la stima finché si ha  $D_ s_v = d_s_v$.
\end{itemize}
$$\texttt{if($D_s_u+w(u,v)< D_s_v)$} \textbf{then} D_s_v \xleftarrow{}D_s_u+w(u,v) $$

\begin{center}
    \includegraphics[width=7cm]{images/Bellman.png}
\end{center}


\end{document}