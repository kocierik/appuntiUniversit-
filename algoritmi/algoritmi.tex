\documentclass[12pt]{article}
\usepackage{amsfonts, amssymb, amsmath}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\parindent 0px

\begin{document}
\title{Algoritmi e strutture dati}
\author{Koci Erik}
\maketitle
\section{Complessità algoritmi}
\begin{enumerate}
    \item \textbf{Costo:} si riferisce al costo di un singolo algoritmo
\item \textbf{Complessità:} si riferisce a più risoluzioni di un algoritmo 
\end{enumerate}
\includegraphics{universe}
il costo di un blocco \textbf{if-then-else} è  $O(max\{f(n),g(n),h(n)\})$ cioè \textbf{O(1)}.

\subsection{Ordini di grandezza}
\begin{enumerate}
    \item $\Theta(f(n))$ se \textbf{cresce tanto quanto f}
    \item $O(f(n))$ \textbf{se la crescita è minore o uguale a f}
    \item $\Omega(f(n))$ \textbf{se la crescita è maggiore o uguale a f}
\end{enumerate}
\subsection{Esercizi}
\begin{itemize}
    \item $1325 n^2 + 12 n + 1 = \theta(n^3)$ FALSO
    \item $76 n^3 = =(n^3)$ VERO
    \item $n^2 log n = O(n^2)$ FALSO
    \item $3^N = O(2^N)$ FALSO
    \item $1^n = O(2^\frac{n}{2})$ FALSO
    \item $2^N+100 = O(2^N)$ VERO
    \item $n = O(n log n)$ VERO
    \item $n^2 = (n log n$ FALSO
    \item $log(n^2) = \Theta(log n)$ VERO
    \item $(n+1)/2 = \Theta(n)$ VERO
    \item $\frac{(n+1)*n}{2} = \Theta(n^2)$ VERO
\end{itemize}
\subsection{Analisi casi}
\begin{enumerate}
    \item \textbf{Caso pessimo} $$T\textsc{worst}(n) = max T(I)$$
    \item \textbf{Caso ottimo} $$T\textsc{best}(n) = min T(I)$$
    \item \textbf{Caso medio} $$T\textsc{avg}(n) = \sum\limits_{} T(I)P(I)$$
\end{enumerate}
\subsection{Algoritmi ordinamento}
\textbf{Selection sort:} scorre tutti gli elementi degli array e si cerca il valore più piccolo scambiando i due valori. Il costo è lineare con il numero di elementi da considerare:
$$T(n) = \Theta (n^2)$$
\textbf{il costo è $\Theta(n^2)$} perchè è presente una funzione min che ogni volta controlla se il numero è il minore.
Le chiamate a \textbf{min} contribuiscono a $n^2$ mentre il resto combacia con $n$
cioè $n^2+n$;\,\,\,\,n viene assorbito. \\

\textbf{Ricerca binaria (ricorsiva):} per utilizzare questo algoritmo devo avere un array ordinato. Cerco il valore andando a verificare sempre nella metà dove mi aspetto che sia presente.
$$T(n) = 1 \,\,\,se\,\, n=0$$
$$T(n) = T(n/2)+1 $$ 
\textbf{equazione di ricorrenza:} ci aiuta a calcolare il costo analizzando una singola ricorsione. \\\\
\subsection{metodo dell'iterazione:} consiste nello sviluppare l'equazione di ricorrenza, per intuirne la soluzione. 

$$T(n) = c_1 + c_2*log (n) = \Theta(log (n))$$
E' presente il \textbf{logaritmo} perchè ogni volta devo \textbf{dimezzare} il tutto in base al numero di elementi. $c_1$ perchè devo eseguire le istruzioni la prima volta.\\

dimostrare che $T(n) = O(n)$
$$T(n = 1) \,\, n==1$$
$$T(T([n/2])+n \,\,\, n>1$$
\subsection{Metodo della sostituzione:} consiste facendo una dimostrazione per induzione. quindi parto dal valore base che è n (esempio 1) e dimostriamo che vale anche per un $n$ più grande. \\\\
\textbf{caso base:} $$T(1) = 1 \leq c x 1$$ \\
\textbf{induzione:}
$$T(n) = T([n/2])+n$$ $$ \leq c[n/2]+n \,\,\,\,\, (ipotesi\,induttiva)$$
$$\leq cn/2+n=\frac{cn+2n}{2} = (c/2+1)n \leq cn$$ \\

\subsection{Master Theorem:} Si consideri la seguente equazione di ricorrenza: \\
$$T(n) = d \,\,\,se\,\, n=1$$
$$T(n) = aT(n/b)+cn^\beta \,\,\,se\,\, n>1$$
e sia: $$\alpha = \frac{log (a)}{log(b)}$$

\textbf{a} numero di chiamate ricorsive\\
\textbf{b} mi dice come partiziono il mio input\\
questi due valori mi danno $\alpha$. \\
$\beta$ mi dice l'esponente che avevo. \\

L'equazione di ricorrenza ha la seguente soluzione:
\begin{enumerate}
    \item $T(n) = \Theta(n^\alpha)$ se $\alpha > \beta$
    \item $T(n) = \Theta(n^\alpha*log(n))$ se $\alpha = \beta$
    \item $T(n) = \Theta(n^\beta)$ se $\alpha < \beta$
\end{enumerate}


Il teorema fondamentale  \textbf{non} si può applicare ad algoritmi ricorsivi che non effettuano \textbf{partizioni bilanciate}.\\
ad esempio non può essere applicato nella risoluzione di fibonacci ricorsivo.\\

Esempio: \\
$$T(n) = 1 \, n<=1$$
$$T(n−1)+T(n − 2)+1 \, n > 2$$
Se le \textbf{partizioni sono bilanciate} conviene utilizzare il   \textbf{Master Theorem}.\\\\
\textbf{partizione bilanciate:} quando facciamo chiamate ricorsive prendo il mio input suddividendolo in parti n/b. Fibonacci non è bilancianto perche abbiamo due chiamate ricorsive diverse. \\

\textbf{L’analisi ammortizzata:} studia il costo medio di una sequenza di operazioni.\\

Sia  \textbf{T(n, k)} il tempo totale richiesto da un algoritmo, nel caso pessimo, per effettuare k operazioni su istanze di lunghezza n.
Definiamo il \textbf{costo ammortizzato} su una sequenza di k operazioni come:
$$T_\alpha(n)=\frac{T(n,k)}{k}$$


\subsection{Algoritmi di visita degli alberi}
Esistono due tipologie di visita:
\begin{itemize}
    \item In profondità (pre-ordine, in-rodine, post-ordine)
    \item In ampiezza
\end{itemize}
Nella visita \textbf{pre-ordine} si parte visitando il nodo della radice per poi passare a visitare tutto il nodo sinistro, risalendo poi andando verso destra.\\\\
Nella visita \textbf{in-ordine} si parte a visitare dal ramo più in basso a sinistra risalendo per poi andare verso destra. \\\\
Nella visita \textbf{post-ordine} vengono prima visitati i nodi più in profondità partendo sempre da sinistra verso destra per poi risalire.\\\\
Nella visita per \textbf{ampiezza} si analizza l'albero a livelli, partendo dalla radice.

\subsection{Alberi AVL}
Un albero $AVL$ è un albero di ricerca (quasi) bilanciato. Questo albero supporta le operazioni di $insert(), delete(), search()$ con costo $O(log n)$ nel \textbf{caso pessimo}.

\subsubsection{Fattore di bilanciamento}
Il fattore di bilanciamento $\beta(v)$ di un novo $v$ è dato dalla differenza tra l'altezza del sottoalbero sinistro e del sottoalbero destro di v:
$$\beta(v) = altezza(v.left)-altezza(v.right)$$
\subsubsection{Bilanciamento in altezza}
Un albero si dice \textbf{bilanciato in altezza} se le altezze dei sottoalberi sinitro e destro di ogni nodo differiscono al più di uno.
$$\beta\leq1$$
\textbf{Definizione:} un albero $AVL$ è un $ABR$ bilanciato in altezza.
\subsubsection{Inserimento e rimozione} 
Inserimenti e rimozioni richiedono di essere modificati per mantenere il bilanciamento dell'albero. \\
L'operazione fondamentale per ribilanciare l'albero è la \textbf{rotazione semplice}.\\
\subsubsection{Rotazione a sinistra}
Per effettuare questa rotazione prendo il nodo problematico e effettuo una rotazione scambiandolo con il successivo ed il nodo scambiato diventerà figlio destro mentre il figlio del nodo precendete diventerà figlio sinistro.

\subsection{Alberi 2-3}
Un albero 2-3 è un albero in cui:
\begin{itemize}
    \item Tutti i percorsi radice-foglia hanno la stessa lunghezza
    \item Le foglie contengono le chiavi (e i dati da memorizzare) e sono ordinate da sinistra verso destra in ordine di chiave crescente.
    \item Ogni nodo interno (non folgia) v ha 2 o 3 figli e mantiene due informazioni \begin{itemize}
        \item S[v], \textbf{chiave massima} nel sottoalbero sinitro (2 o 3 figli)
        \item M[v], \textbf{chiave massima} nel sottoalbero centrale (3 figli)
    \end{itemize}
    \item Distribuzione dei valori $k$ delle chiavi nei sottoalberi: \begin{itemize}
        \item Sinistro $k \leq S[v]$
        \item Centro $S[v] < k \leq M[v]$
        \item Destro $k > M[v]$
    \end{itemize}
\end{itemize}


\subsection{Tabelle Hash}
Le \textbf{tabelle hash} hanno una implementazione basata su una chiave $k$ e array.\\
Per ottenere la chiave sono presenti diverse tecniche di calcolo.\\
Ricapitolando, per realizzare una tabella hash efficiente abbiamo bisogno di:
\begin{itemize}
    \item Un vettore
    \item Una funzione has calcolabile velocemente e che garantisca una buona distribuzione delle chiavi nel vettore
    \item Un meccanisco per gestire le collisioni 
\end{itemize}
\subsubsection{Problema delle collisioni}
Una funzione hash $h$ si dice \textbf{perfetta} se è iniettiva:
$$\forall u, \in U: u \neq v \xrightarrow{} h(u) \neq h(v)$$
Se le collisioni sono inevitabili, è necessario trovare un metodo che le minimizzi, distribuendo \textbf{uniformemente} le chaivi negli indici della tabella hash.
\subsubsection{Funzioni hash}
E' necessario fare una premessa; nelle funzioni hash è \textbf{sempre possibile} trasformare una chiave complessa in un numero, (conversione in binario).
\subsubsection{Metodo dell'estrazione}
Le caratteristiche di questo metodo sono:
\begin{itemize}
    \item Usa solo una parte della chiave
    \item Si seleziona una sottosequenza di $p$ bit, con $m = 2^p$
    \item Solitamente dalle posizioni centrali
\end{itemize}
\textbf{esempio:} Verranno prese le cifre centrali 101000
\begin{center}
 bin(“beer”) = 000010 000101 000101 010010 
\end{center}
\begin{itemize}
    \item \textbf{Vantaggi:} molto veloce da calcolare
    \item \textbf{Svantaggi} rischio collisioni più alto di altri metodi
\end{itemize}

\subsubsection{Metodo della divisione}
Basata sul resto della divisione per $m$:
\begin{itemize}
    \item \textbf{Vantaggio:} molto veloce
    \item \textbf{Svantaggio:} Suscettibile a speficifi valori di $m$. Per risolvere questo problema bisogna scegliere $m$ come numero primo non troppo vicino ad una potenza di 2.
\end{itemize}
\textbf{Esempio:}
$$m=12, k=100 \xrightarrow{} h(k)=4$$
\subsubsection{Metodo della moltiplicazione}
Basato sulla moltiplicazione e il resto del numero
\begin{enumerate}
    \item Sia $A$ una costante, $0 < A < 1$
    \item Moltiplichiamo $k$ per $A$ e prendiamo la parte frazionaria
    \item Moltiplichiamo quest'ultima per $m$ e prendiamo la parte intera
\end{enumerate}
\textbf{Esempi:}
$$m = , k = 3, A = 0.8 \xrightarrow{} h(k) = 2$$
$$m = 1000,k = 123, A \approx 0.6180339887... \xrightarrow{} h(k) = 18$$
\begin{itemize}
    \item \textbf{Svantaggi:} lento (più lento del metodo di divisione)
    \item \textbf{Vantaggi} Il valore di $m$ non è critico
    \item \textbf{Come scegliere A?} $A \approx (\sqrt{5}-1)/2 = 0.61803...$ \textbf{(Knuth)}
\end{itemize}
\subsubsection{Metodo della codifica algebrica}
Metodo utilizzando dal compilatore java basato su espressioni algebriche:
$$h(k) = (k_n x^n + k_n_-_1 x^n^-^1 + ... + k_1 x + k_0) mod \hspace{0.1cm}m$$
$$k = k_n k_n_-_1 ... k_1k_0$$
Dove $k_0, k_1...$ possono essere, ad esempio, i bit della \textbf{codifica binaria} di $k$, oppure i \textbf{codici ascii} dei singoli caratteri di $k$.\\ $x$ è un valore \textbf{costante}.
\begin{itemize}
    \item \textbf{Vantaggi:} dipende da tutti i bit/caratteri della chiave
    \item \textbf{Svantaggi:} $n$ addizioni e $n*(n-1)/2$ prodotti
\end{itemize}
\subsubsection{Problema delle collisioni}
Attraverso questi metodi elencati precedentemente siamo riusciti a ridurre il numero di collisioni, ma senza eliminarle.\\\\
Per risolvere questo problema la  \textbf{complessita computazionale potrebbe aumentare a $n$}, possono essere utilizzate le seguenti tecniche:
\begin{enumerate}
    \item \textbf{Concatenamento}
    \item \textbf{Indirizzamento aperto}
\end{enumerate}
\subsubsection{concatenamento}
     Nella tecnica di  \textbf{concatenameto} gli elementi con lo stesso valore hash  $h$ vengono memorizzati in una lista concatenata (linked list).\\

Il \textbf{fattore di carico} è dato dal rapporto tra numero di elementi memorizzati e dimensioni della tabella. \\\\
La \textbf{complessità} del concatenamento è la seguente:
\begin{itemize}
    \item insert: $\Theta(1)$
    \item search: $\Theta(n)$
    \item delete: $\Theta(n)$
\end{itemize}
\subsubsection{indirizzamento aperto}
L'idea è quella di memorizzare tutte le chiavi nella tabella stessa, ed ogni slot contiene una chiave oppure $null$. \\\\
\textbf{Inserimento:} se lo slot prescelto è utilizzato, si cerca uno slot alternativo. \\\\
\textbf{Ricerca:} si cerca nello slot prescelto, epoi negli slot alternativi fino a quando non si trova la chiave oppure $null$.\\\\ 
Vengono utilizzati diversi algoritmi di indirizzamento, per esempio i seguenti:\\\\
\textbf{Ispezione lineare}\\
Il primo elemento determina l'intera sequenza. In questo modo si ottengono \textbf{lunghe sottosequenze}.
$$h(k,i) = (h' (k) + 1)$$
\\
\textbf{Ispezione quadratica:}\\
L'ispezione iniziale è in $h'(k)$, mentre le succesive hanno un offset che dipende da una \textbf{funzione quadratica nel numero di ispezione}.
$$h'(k) + c_1i + c_2 i^2$$
\\
\textbf{Doppio hashing:}\\
Formato da \textbf{due funzioni ausiliari} di cui la prima $h_1$ fornisce la prima ispezione, mentre $h_2$ fornisce l'offset delle succesive ispezioni.
$$h(k,i) = (h_1(k) + i h_2(k))$$

\subsubsection{Conclusioni hash table}
Usare funzioni hash $h(k)$ che producano valori il più possibile uniformemente distribuiti è molto importante perchè altrimenti potremmo arrivare ad una complessità computazionale pari a $O(n)$.\\\\
\textbf{Problemi con hashing:}
\begin{itemize}
    \item Scarsa locality of reference (cache miss)
    \item In base all'implementazione è in genere difficile ottenere le chiavi in ordine
    \item Sebbene il costo medio per operazione sia basso, la singola operazione può risultare molto costosa, ad esempio se occorre ridimensionare la tabell ae redistribuire le chiavi.
\end{itemize}
\newpage
\section{Scelta degli algoritmi}
A seconda delle operazioni da eseguire è necessario adattare diverse tecniche di implimentazione di un algoritmo.
\subsubsection{Implementazione su un vettore ordinato} 
Questo tipo di ricerca ha un costo computazionale basso nel caso in cui volessimo \textbf{ricercare degli elementi}.
Costi computazionali:
\begin{itemize}
    \item Ricerca $O(log\hspace{0.1cm}n)$
    \item Inserimento $O(n)$
    \item Eliminazione $O(n)$
\end{itemize}
\subsubsection{Implementazione su liste concatenate non ordinate}
Questo implementazione converrebbe utilizzarla nel caso in cui volessimo \textbf{aggiungere o eliminare degli elementi}.
Costi computazionali:
\begin{itemize}
    \item Ricerca $O(n)$
    \item Inserimento $O(1)$
    \item Eliminazione $O(n)$
\end{itemize}

\subsubsection{Implementazione alberi ABR}
Implementazione basata su alberi binari.
Costi computazionali:
\begin{itemize}
    \item Ricerca $O(h)$
    \item Inserimento $O(h)$
    \item Eliminazione $O(h)$
\end{itemize}

\subsubsection{Implementazione alberi AVL}
Implementazione basata su alberi binari a altezza equivalente.
Costi computazionali:
\begin{itemize}
    \item Ricerca $O(log \hspace{0.1cm}n)$
    \item Inserimento $O(log \hspace{0.1cm}n)$
    \item Eliminazione $O(log \hspace{0.1cm}n)$
\end{itemize}

\subsubsection{Implementazione alberi 2-3}
Implementazione basata su alberi binari ordinati con chiavi massime.
Costi computazionali:
\begin{itemize}
    \item Ricerca $O(log \hspace{0.1cm}n)$
    \item Inserimento $O(log \hspace{0.1cm}n)$
    \item Eliminazione $O(log \hspace{0.1cm}n)$
\end{itemize}

\subsubsection{Hash table}
Implementazione basata su array, dove l'elemento con chiave $k$ è memorizzato nel $k-esimo$ "slot" dell'array.

\begin{itemize}
    \item Ricerca caso medio $O(1)$ \hspace{2cm} Ricerca caso pessimo $O(n)$
    \item Inserimento caso medio $O(1)$ \hspace{1.2cm} Inserimento caso pessimo $O(n)$
    \item Eliminazione caso medio $O(1)$
    \hspace{1cm} Eliminazione caso pessimo $O(n)$
\end{itemize}

\subsection{Riepilogo}
\begin{center}
\includegraphics[width=12cm]{images/complessita.png}
\end{center}


\section{Algoritmi di ordinamento}
\subsubsection{ordinamento in loco:}
L'algoritmo permuta gli elementi direttamente nell'array
originale, senza usare un altro array di appoggio.
\subsubsection{ordinamento stabile:}
L'algoritmo preserva l'ordine con cui elementi con la stessa
chiave compaiono nell'array originale.
\subsection{Selection sort}
Cerca il minimo in $A[k+1..n]$ e spostalo in posizione $k+1$.\\
La complessita di questo algoritmo è pari a:
$$O(n^2)$$
\begin{center}
\includegraphics[width=5cm]{images/selectionSort.png}
\end{center}
\subsection{Insertion sort}
Inserisco l'elemento di posizione k+1 nella \textbf{posizione corretta} all'interno dei primi k elementi ordinati. Al termine del passo k, il vettore ha le prime k componenti  ordinate.\\
Il costo computazionale di questo algoritmo è:
$$O(n^2)$$
\begin{center}
\includegraphics[width=5cm]{images/insertionSort.png}
\end{center}
\newpage
\subsection{Bubble sort}
Ad ogni scansione scambia le coppie di elementi adiacenti che non sono nell'ordine corretto.\\
Ad ogni scansione \textbf{scambia le coppie di elementi adiacenti}.
Dopo la prima scansione, l'elemento massimo occupa l'ultima posizione, dopo la k-esima scansione, i k elementi massimi
occupano la posizione corretta in fondo all'array.
Nel caso $pessimo--ottimo$ bubble Sort ha costo:
$$\Theta(n^2) \hspace{1cm} \Theta(n)$$
\begin{center}
\includegraphics[width=7cm]{images/bubbleSort1.png}
\includegraphics[width=7cm]{images/bubbleSort2.png}
\includegraphics[width=7cm]{images/bubbleSort3.png}
\end{center}
\newpage
\subsection{Quick sort}
Scegli un elemento x del vettore v, e \textbf{partiziona il vettore in due parti} considerando gli elementi $\leq x$ e quelli $>x$\\
Ordina ricorsivamente le due parti.\\
Restituisci il risultato concatenando le due parti ordinate.
\subsubsection{Partizionamento}
Manteniamo due indici, inf e sup, che vengono fatti \textbf{scorrere dalle estremità} del vettore verso il centro. Quando entrambi (inf e sup) non possono essere fatti avanzare verso il centro, si \textbf{scambia} $A[inf]$ e $A[sup]$.\\
Il costo quick sort: Dipende dal partizionamento: 
\begin{center}
\textbf{caso peggiore:} $\Theta(n^2)$ \hspace{2cm} \textbf{caso migliore:} $\Theta(n \hspace{0.1cm}log \hspace{0.1cm}n)$
\end{center}
\begin{center}
\includegraphics[width=7cm]{images/quickSort.png}
\end{center}

\newpage

\subsection{Merge Sort}

Questo algoritmo \textbf{divide} $A[]$ in \textbf{due meta'} $A1[]$ e $A2[]$(senza permutare) di dimensioni uguali;\\
Applica \textbf{ricorsivamente} Merge Sort a
$A1[]$ e $A2[]$.\\
\textbf{Fonde} (merge) gli array ordinati $A1[]$ e
$A2[]$ per ottenere l'array $A[]$ ordinato.
\begin{center}
\includegraphics[width=9cm]{images/mergeSort.png}
\end{center}

\subsection{Heapsort}
Funzionamento: 
\begin{enumerate}
    \item Costruire un \textbf{max-heap} a partire dal vettore A[] originale, mediante l'operazione \textbf{heapify()}
    \item Estrarre il \textbf{massimo} $( findMax() + deleteMax() )$
    \item \textbf{Inserire} il massimo in ultima posizione di A[].
    \item \textbf{Ripetere} il punto 2. finché lo heap diventa vuoto
\end{enumerate}
\newpage
\subsubsection{Albero binario perfetto}
Un albero binario è \textbf{perfetto} se:


\begin{itemize}
    \item Tutte le foglie hanno la stessa altezza h
    \item Nodi interni hanno grado 2
\end{itemize}
 Un albero perfetto ha altezza $h\hspace{0.1cm} \simeq log \hspace{0.1cm} N$  \\
 Il numero di nodi è $ N = nodi = 2^h^+^1 -1$
 
 \subsubsection{Albero binario completo}
 Un albero binario è \textbf{completo} se:
\begin{itemize}
    \item Tutte le foglie hanno profondità h o h-1
    \item Tutti i nodi a livello h sono “accatastati” a
    sinistra
    \item Tutti i nodi interni hanno grado 2, eccetto al più uno
\end{itemize}
\subsubsection{Max-heap}
Un albero binario completo è un albero \textbf{max-heap} sse:
\begin{itemize}
    \item Ad ogni nodo i viene associato un valore $A[i]$
    \item $A[Parent(i)] \geq A[i]$
\end{itemize}
\subsubsection{Min-heap}
Un albero binario completo è un albero \textbf{min-heap} sse:
\begin{itemize}
    \item Ad ogni nodo i viene associato un valore $A[i]$
    \item $A[Parent(i)] \leq A[i]$
\end{itemize}
\newpage

\subsection{Operazioni su array heap}
\subsubsection{findMax()} 
Individua il valore massimo contenuto in uno heap.\\
Il massimo è sempre la radice, ossia A[1].\\
L'operazione ha costo $\Theta(1)$.

\subsubsection{fixHeap()}
Ripristinare la proprietà di max-heap.\\
Supponiamo di rimpiazzare la radice $A[1]$ di un max-heap
con un valore qualsiasi, vogliamo fare in modo che $A[]$ diventi nuovamente uno heap.

\subsubsection{heapify()}
Costruire uno heap a partire da un array privo di alcun ordine.

\subsubsection{deleteMax()}
Rimuovi l'elemento massimo da un maxheap $A[]$.

\section{Selezione del k-esimo}
Consideriamo il seguente problema:\\
\textbf{Selezione del k-esimo minimo}: dato un array $A[1..n]$ di valori
distinti e un valore $1 \leq k \leq n$, trovare l'elemento che è
maggiore di esattamente $k-1$ elementi.\\
\textbf{Mediano}: il valore che occuperebbe la posizione $(n/2)$ se
l'array fosse ordinato.\\
I \textbf{motori di ricerca} producono molti risultati a fronte di
una singola query. I risultati vengono mostrati in pagine, in ordine
decrescente di rilevanza. È \textbf{inutile ordinare tutti i risultati} in base alla rilevanza.\\
Verifichiamo ora i costi computazionali dei singoli casi:\\
\textbf{Ricerca del minimo:}\\
$$T(n) = n-1 = \Theta(n)$$
\textbf{Ricerca del secondo minimo:}\\
$$T(n) = 2n-3 = \Theta(n)$$
\textbf{Selezione del k-esimo elemento:}\\
$$T(n) = \Theta(kn)$$
\textbf{Selezione del valore mediano:}\\
$$T(n) = O(n + k\hspace{0.1cm} log\hspace{0.1cm} n) = O(n + (n/2) log \hspace{0.1cm} n) = O(n \hspace{0.1cm} log \hspace{0.1cm} n)$$
\subsubsection{Adattamento di quicksort al
problema della selezione:}
In questo modo divido il mio array in più partizioni, andando a eliminare quelle inutili.
\begin{center}
\includegraphics[width=8cm]{images/partizionamento1.png}
\includegraphics[width=8cm]{images/partizionamento2.png}
\end{center}
\subsubsection{Analisi dell'algoritmo quickSelect()}
\textbf{Costo nel caso ottimo:}\\
$$T(n) = T(n/2) + n = \Theta(n)$$
\textbf{Costo nel caso pessimo:}\\
$$T(n) = T(n-1) + n = \Theta(n^2)$$
\textbf{Costo nel caso medio:}\\
$$T(n) \leq 4n$$
\end{document}